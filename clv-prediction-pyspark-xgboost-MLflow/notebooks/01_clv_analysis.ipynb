{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Lifetime Value (CLV) Prediction with PySpark, XGBoost, and MLflow\n",
    "\n",
    "In this mini project, we use the **Online Retail II** dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/502/online+retail+ii) to predict 6‑month customer lifetime value (CLV) for a UK-based online retail business. We treat this as a supervised regression problem and focus on reproducible, production-minded workflows.\n",
    "\n",
    "**Business goal:** given a customer's historical transactions (recency, frequency, monetary value, product mix, etc.), estimate their **future 6‑month revenue** so marketing and retention teams can prioritize high‑value customers.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Environment Setup & Imports\n",
    "\n",
    "_(In this section we will import PySpark, pandas, XGBoost, scikit-learn, MLflow, and plotting libraries.)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "\nXGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed\n    - vcomp140.dll or libgomp-1.dll for Windows\n    - libomp.dylib for Mac OSX\n    - libgomp.so for Linux and other UNIX-like OSes\n    Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n\n  * You are running 32-bit Python on a 64-bit OS\n\nError message(s): [\"dlopen(/Users/taravat/Documents/Cursor/mini-projects/clv-prediction-pyspark-xgboost-MLflow/.venv/lib/python3.11/site-packages/xgboost/lib/libxgboost.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\\n  Referenced from: <636BF463-1886-392D-B8B3-6011C44DCEE9> /Users/taravat/Documents/Cursor/mini-projects/clv-prediction-pyspark-xgboost-MLflow/.venv/lib/python3.11/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file)\"]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# PySpark\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     col,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28msum\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m spark_sum,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     first,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwindow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Window\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Environment Setup & Imports\n",
    "import os\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    sum as spark_sum,\n",
    "    count as spark_count,\n",
    "    countDistinct,\n",
    "    max as spark_max,\n",
    "    min as spark_min,\n",
    "    avg as spark_avg,\n",
    "    stddev,\n",
    "    datediff,\n",
    "    lit,\n",
    "    first,\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Experiment tracking\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting style\n",
    "sns.set(style=\"whitegrid\", context=\"notebook\")\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "EXCEL_PATH = os.path.join(DATA_DIR, \"online_retail_II.xlsx\")\n",
    "CSV_PATH = os.path.join(DATA_DIR, \"transactions.csv\")\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Excel path:\", EXCEL_PATH)\n",
    "print(\"CSV path:\", CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Spark Session & Data Loading\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"clv-prediction\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# Create a CSV from the Online Retail II Excel file (one-time, idempotent)\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    print(\"CSV not found, creating from Excel...\")\n",
    "\n",
    "    # Read all sheets from the Excel file and concatenate\n",
    "    excel_sheets = pd.read_excel(EXCEL_PATH, sheet_name=None)\n",
    "    raw_df = pd.concat(excel_sheets.values(), ignore_index=True)\n",
    "\n",
    "    # Drop completely empty rows\n",
    "    raw_df = raw_df.dropna(how=\"all\")\n",
    "\n",
    "    # Remove cancelled invoices (InvoiceNo starting with 'C')\n",
    "    raw_df = raw_df[~raw_df[\"InvoiceNo\"].astype(str).str.startswith(\"C\")]\n",
    "\n",
    "    # Rename columns to snake_case / modeling-friendly names\n",
    "    raw_df = raw_df.rename(\n",
    "        columns={\n",
    "            \"InvoiceNo\": \"invoice_id\",\n",
    "            \"StockCode\": \"product_id\",\n",
    "            \"Description\": \"description\",\n",
    "            \"Quantity\": \"quantity\",\n",
    "            \"InvoiceDate\": \"invoice_date\",\n",
    "            \"UnitPrice\": \"unit_price\",\n",
    "            \"CustomerID\": \"customer_id\",\n",
    "            \"Country\": \"country\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    raw_df.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"Saved combined CSV to {CSV_PATH}\")\n",
    "else:\n",
    "    print(\"Using existing CSV at:\", CSV_PATH)\n",
    "\n",
    "# Load CSV into Spark\n",
    "\n",
    "df = spark.read.csv(CSV_PATH, header=True, inferSchema=True)\n",
    "\n",
    "print(\"Raw Spark schema:\")\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Cleaning & Basic EDA (PySpark)\n",
    "\n",
    "_In this section we will load the Online Retail II dataset into Spark, clean cancellations/returns, filter invalid rows, and explore basic distributions (invoices, customers, countries, etc.)._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_timestamp\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Cast invoice_date to timestamp and create amount column\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoice_date\u001b[39m\u001b[38;5;124m\"\u001b[39m, to_timestamp(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoice_date\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      8\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamount\u001b[39m\u001b[38;5;124m\"\u001b[39m, col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquantity\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m*\u001b[39m col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munit_price\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Basic filtering: non-null customer_id and positive amount\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# 3. Data Cleaning & Basic EDA (PySpark)\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Cast invoice_date to timestamp and create amount column\n",
    "\n",
    "df = df.withColumn(\"invoice_date\", to_timestamp(\"invoice_date\"))\n",
    "df = df.withColumn(\"amount\", col(\"quantity\") * col(\"unit_price\"))\n",
    "\n",
    "# Basic filtering: non-null customer_id and positive amount\n",
    "\n",
    "df = df.filter(col(\"customer_id\").isNotNull())\n",
    "df = df.filter(col(\"amount\") > 0)\n",
    "\n",
    "summary = df.select(\n",
    "    countDistinct(\"customer_id\").alias(\"n_customers\"),\n",
    "    countDistinct(\"invoice_id\").alias(\"n_orders\"),\n",
    "    spark_sum(\"amount\").alias(\"total_revenue\"),\n",
    ")\n",
    "\n",
    "print(\"High-level summary after basic cleaning:\")\n",
    "summary.show()\n",
    "\n",
    "print(\"Row count after cleaning:\", df.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train / Prediction Window Definition (CLV Target Construction)\n",
    "\n",
    "We will:\n",
    "- Define an **observation window** (history period) per customer to build features.\n",
    "- Define a **6‑month prediction window** after the observation end date.\n",
    "- Compute **future revenue** per customer over the prediction window as the CLV target.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 4. Train / Prediction Window Definition (CLV Target Construction)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Find the maximum invoice_date in the cleaned data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m max_date_row \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mselect(spark_max(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvoice_date\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_date\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mcollect()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m max_date \u001b[38;5;241m=\u001b[39m max_date_row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMax invoice_date:\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_date)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# 4. Train / Prediction Window Definition (CLV Target Construction)\n",
    "\n",
    "# Find the maximum invoice_date in the cleaned data\n",
    "max_date_row = df.select(spark_max(\"invoice_date\").alias(\"max_date\")).collect()[0]\n",
    "max_date = max_date_row[\"max_date\"]\n",
    "print(\"Max invoice_date:\", max_date)\n",
    "\n",
    "# Define cutoff date 6 months (180 days) before the max date\n",
    "cutoff_date = max_date - timedelta(days=180)\n",
    "print(\"Cutoff date (6 months before max):\", cutoff_date)\n",
    "\n",
    "# Split into history (features) and future (CLV target) windows\n",
    "history_df = df.filter(col(\"invoice_date\") <= lit(cutoff_date))\n",
    "future_df = df.filter(col(\"invoice_date\") > lit(cutoff_date))\n",
    "\n",
    "print(\"History rows:\", history_df.count())\n",
    "print(\"Future rows:\", future_df.count())\n",
    "\n",
    "# Compute future 6-month revenue (CLV) per customer\n",
    "clv_df = future_df.groupBy(\"customer_id\").agg(\n",
    "    spark_sum(\"amount\").alias(\"future_6m_revenue\")\n",
    ")\n",
    "\n",
    "print(\"Sample of CLV labels:\")\n",
    "clv_df.orderBy(col(\"future_6m_revenue\").desc()).show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering with PySpark (RFM & More)\n",
    "\n",
    "Here we will create customer-level features such as:\n",
    "- **Recency**: days since last purchase in the observation window\n",
    "- **Frequency**: number of transactions\n",
    "- **Monetary value**: total and average revenue\n",
    "- Product/category diversity, country, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Feature Engineering with PySpark (RFM & more)\n",
    "\n",
    "customer_agg = history_df.groupBy(\"customer_id\").agg(\n",
    "    spark_count(\"invoice_id\").alias(\"num_orders\"),\n",
    "    spark_sum(\"amount\").alias(\"total_spent\"),\n",
    "    spark_avg(\"amount\").alias(\"avg_order_value\"),\n",
    "    stddev(\"amount\").alias(\"order_amount_std\"),\n",
    "    countDistinct(\"product_id\").alias(\"num_unique_products\"),\n",
    "    spark_min(\"invoice_date\").alias(\"first_order_date\"),\n",
    "    spark_max(\"invoice_date\").alias(\"last_order_date\"),\n",
    "    first(\"country\").alias(\"country\"),\n",
    ")\n",
    "\n",
    "# Recency & tenure\n",
    "customer_agg = customer_agg.withColumn(\n",
    "    \"recency_days\",\n",
    "    datediff(lit(cutoff_date), col(\"last_order_date\")),\n",
    ").withColumn(\n",
    "    \"tenure_days\",\n",
    "    datediff(col(\"last_order_date\"), col(\"first_order_date\")),\n",
    ")\n",
    "\n",
    "# Join with CLV labels (future 6-month revenue)\n",
    "features_df = (\n",
    "    customer_agg.join(clv_df, on=\"customer_id\", how=\"left\").fillna({\"future_6m_revenue\": 0.0})\n",
    ")\n",
    "\n",
    "# Replace null stddev with 0\n",
    "features_df = features_df.fillna({\"order_amount_std\": 0.0})\n",
    "\n",
    "print(\"Feature dataframe sample:\")\n",
    "features_df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train/Test Split at Customer Level\n",
    "\n",
    "We will split customers into train and test sets (e.g., 80/20) to avoid data leakage across time windows and transactions for the same customer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Train/Test Split at Customer Level\n",
    "\n",
    "features_pd = features_df.toPandas()\n",
    "print(\"Pandas feature shape:\", features_pd.shape)\n",
    "\n",
    "target_col = \"future_6m_revenue\"\n",
    "id_col = \"customer_id\"\n",
    "\n",
    "cols_to_drop = [target_col, id_col, \"first_order_date\", \"last_order_date\"]\n",
    "feature_cols = [c for c in features_pd.columns if c not in cols_to_drop]\n",
    "\n",
    "X = features_pd[feature_cols].copy()\n",
    "y = features_pd[target_col].values\n",
    "\n",
    "# One-hot encode country if present\n",
    "if \"country\" in X.columns:\n",
    "    X = pd.get_dummies(X, columns=[\"country\"], dummy_na=True)\n",
    "\n",
    "# Simple missing value handling\n",
    "X = X.fillna(0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Modeling CLV with XGBoost + MLflow\n",
    "\n",
    "- Train an XGBoost regressor on engineered features.\n",
    "- Use MLflow to track experiments (hyperparameters, metrics, artifacts).\n",
    "- Log the best model for potential deployment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Modeling CLV with XGBoost + MLflow\n",
    "\n",
    "mlflow.set_experiment(\"clv-prediction\")\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": 400,\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "with mlflow.start_run(run_name=\"xgb_clv_baseline\"):\n",
    "    model = XGBRegressor(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    mlflow.log_params(params)\n",
    "    mlflow.log_metric(\"mae\", float(mae))\n",
    "    mlflow.log_metric(\"r2\", float(r2))\n",
    "\n",
    "    mlflow.xgboost.log_model(model, artifact_path=\"model\")\n",
    "\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"R2: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation & Business Interpretation\n",
    "\n",
    "- Evaluate regression performance (RMSE, MAE, R²).\n",
    "- Analyze feature importance and segments (e.g., top‑decile customers by predicted CLV).\n",
    "- Discuss how marketing/retention teams could use these insights.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Evaluation & Business Interpretation (basic plots)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "plt.xlabel(\"Actual CLV (future_6m_revenue)\")\n",
    "plt.ylabel(\"Predicted CLV\")\n",
    "plt.title(\"Predicted vs Actual CLV\")\n",
    "\n",
    "min_val = min(y_test.min(), y_pred.min())\n",
    "max_val = max(y_test.max(), y_pred.max())\n",
    "plt.plot([min_val, max_val], [min_val, max_val], \"r--\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature importances\n",
    "importances = model.feature_importances_\n",
    "sorted_idx = np.argsort(importances)[-20:]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.barh(range(len(sorted_idx)), importances[sorted_idx])\n",
    "plt.yticks(range(len(sorted_idx)), X_train.columns[sorted_idx])\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.title(\"XGBoost Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Production Mindset & Next Steps\n",
    "\n",
    "Ideas for future work:\n",
    "- Move heavy feature engineering into dedicated PySpark jobs (e.g., in `scripts/preprocess.py`).\n",
    "- Schedule regular CLV refreshes (daily/weekly) with updated transactions.\n",
    "- Serve predictions via an API or batch exports to marketing tools.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for future preprocessing helpers (if needed)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Notebook imported as a module; no action taken.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
